{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pVXOYNmls5Uv",
        "LZXcFYSBs550",
        "hMJF8ZpbyLt0",
        "j62FTCRNyOhn",
        "kNMqVXUxg9pQ",
        "Ndj4MZ4-hARb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations/Préparations"
      ],
      "metadata": {
        "id": "pVXOYNmls5Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install"
      ],
      "metadata": {
        "id": "LZXcFYSBs550"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jMZrJSJHsd0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4701ef-f30f-46c7-9fa0-a7b403aefeab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting importlib-metadata==4.13.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata==4.13.0) (3.15.0)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.1.0\n",
            "    Uninstalling importlib-metadata-6.1.0:\n",
            "      Successfully uninstalled importlib-metadata-6.1.0\n",
            "Successfully installed importlib-metadata-4.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==65.5.0\n",
            "  Downloading setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.1\n",
            "    Uninstalling setuptools-67.6.1:\n",
            "      Successfully uninstalled setuptools-67.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-65.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install importlib-metadata==4.13.0\n",
        "!pip install setuptools==65.5.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/osigaud/bbrl_gym\n",
        "\n",
        "!pip install git+https://github.com/osigaud/bbrl"
      ],
      "metadata": {
        "id": "lT6bG-resj5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ec9742-1606-4e6f-b9b7-ccb30b65ee48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl_gym\n",
            "  Cloning https://github.com/osigaud/bbrl_gym to /tmp/pip-req-build-qgtlw6os\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl_gym /tmp/pip-req-build-qgtlw6os\n",
            "  Resolved https://github.com/osigaud/bbrl_gym to commit 5557075ecd7d4171ac0c21be3c69a94bcae655a9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mazemdp>=0.7.3\n",
            "  Downloading mazemdp-0.8.0-py3-none-any.whl (15 kB)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.9/dist-packages (from bbrl-gym==1.2.5) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.21.0->bbrl-gym==1.2.5) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (0.11.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.16.0)\n",
            "Building wheels for collected packages: bbrl-gym, gym, box2d-py\n",
            "  Building wheel for bbrl-gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bbrl-gym: filename=bbrl_gym-1.2.5-py3-none-any.whl size=17602 sha256=575bcc05dda135741db6bd27da91b8b1083ee8e73ef4235438893547eb8b532f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pn6v173w/wheels/4a/86/7e/764dd96df41af554f7e3de22278b3d2e7e7300de5f75087525\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Successfully built bbrl-gym\n",
            "Failed to build gym box2d-py\n",
            "Installing collected packages: swig, box2d-py, gym, mazemdp, bbrl-gym\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed bbrl-gym-1.2.5 box2d-py-2.3.8 gym-0.21.0 mazemdp-0.8.0 swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl\n",
            "  Cloning https://github.com/osigaud/bbrl to /tmp/pip-req-build-jwy6uefh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl /tmp/pip-req-build-jwy6uefh\n",
            "  Resolved https://github.com/osigaud/bbrl to commit cb2c22b82bfedfb04d8232ba432cdbd798fd797a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: bbrl\n",
            "  Building wheel for bbrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bbrl: filename=bbrl-0.1.10.dev4+gcb2c22b-py3-none-any.whl size=55867 sha256=229a53c1dae0d4bc0dbafa9614e07d1878c68309f810ef023c4016d8d0235e06\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u83y59cr/wheels/36/90/b2/6699f093f5054006fa568447050d45939efe7045bf0c569e04\n",
            "Successfully built bbrl\n",
            "Installing collected packages: bbrl\n",
            "Successfully installed bbrl-0.1.10.dev4+gcb2c22b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "id": "JdZ9ukya13_F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "41aeb33b-036d-48be-dc93-88f64ebb0f1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from omegaconf) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=2506f6404cf8d4d310315720e619751327135b52d3b9a06de485bc5d569d5f60\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "hMJF8ZpbyLt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bbrl.workspace import Workspace\n",
        "from bbrl import get_class, get_arguments, instantiate_class\n",
        "\n",
        "import bbrl_gym\n",
        "import gym\n",
        "\n",
        "from bbrl.agents.agent import Agent\n",
        "from bbrl.agents import Agents, TemporalAgent\n",
        "from bbrl.agents.gymb import NoAutoResetGymAgent\n",
        "\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "from bbrl.utils.chrono import Chrono\n",
        "\n",
        "from bbrl.visu.visu_policies import plot_policy\n",
        "from bbrl.visu.visu_critics import plot_critic\n",
        "from bbrl.visu.common import final_show\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from omegaconf import DictConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib\n",
        "import os\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import copy"
      ],
      "metadata": {
        "id": "P5EkdUXfslay"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "j62FTCRNyOhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MazeMDPContinuousWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Specific wrapper to turn the Tabular MazeMDP into a continuous state version\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(MazeMDPContinuousWrapper, self).__init__(env)\n",
        "        # Building a new continuous observation space from the coordinates of each state\n",
        "        high = np.array(\n",
        "            [\n",
        "                env.coord_x.max() + 1,\n",
        "                env.coord_y.max() + 1,\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        low = np.array(\n",
        "            [\n",
        "                env.coord_x.min(),\n",
        "                env.coord_y.min(),\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.observation_space = gym.spaces.Box(low, high)\n",
        "\n",
        "    def is_continuous_state(self):\n",
        "        # By contrast with the wrapped environment where the state space is discrete\n",
        "        return True\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        x = self.env.coord_x[obs]\n",
        "        y = self.env.coord_y[obs]\n",
        "        xc = x + random.random()\n",
        "        yc = y + random.random()\n",
        "        continuous_obs = [xc, yc]\n",
        "        return continuous_obs\n",
        "\n",
        "    def step(self, action):\n",
        "        # Turn the discrete state into a pair of continuous coordinates\n",
        "        # Take the coordinates of the state and add a random number to x and y to\n",
        "        # sample anywhere in the [1, 1] cell...\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        x = self.env.coord_x[next_state]\n",
        "        y = self.env.coord_y[next_state]\n",
        "        xc = x + random.random()\n",
        "        yc = y + random.random()\n",
        "        next_continuous = [xc, yc]\n",
        "        return next_continuous, reward, done, info\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, cfg):\n",
        "        self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "    def add_log(self, log_string, loss, epoch):\n",
        "        self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "    # Log losses\n",
        "    def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n",
        "        self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "        self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "        self.add_log(\"actor_loss\", actor_loss, epoch)\n",
        "\n",
        "    def log_reward_losses(self, rewards, nb_steps):\n",
        "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
        "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
        "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
        "        self.add_log(\"reward/median\", rewards.median(), nb_steps)\n",
        "\n",
        "def format_num(num, pos):\n",
        "    # Pos is a required parameter, but it is not used\n",
        "    magnitude = 0\n",
        "    labels = [\"\", \"K\", \"M\", \"G\"]\n",
        "    while abs(num) >= 1e3:\n",
        "        magnitude += 1\n",
        "        num /= 1e3\n",
        "\n",
        "    return f\"{num:.1f}{labels[magnitude]}\"\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "\n",
        "    def plot_reward(\n",
        "        self,\n",
        "        algo_name,\n",
        "        env_name,\n",
        "        mode=\"mean\",\n",
        "        prefix=\"\",\n",
        "        suffix=\".pdf\",\n",
        "        save_fig=True,\n",
        "        save_dir=\"./plots/\",\n",
        "    ):\n",
        "        _, ax = plt.subplots(figsize=(9, 6))\n",
        "        formatter = FuncFormatter(format_num)\n",
        "\n",
        "        colors = [\"#09b542\", \"#008fd5\", \"#fc4f30\", \"#e5ae38\", \"#e5ae38\", \"#810f7c\"]\n",
        "        color = colors[0]\n",
        "\n",
        "        loader = RewardLoader(self.steps_filename, self.rewards_filename)\n",
        "        steps, rewards = loader.load()\n",
        "        print(steps, rewards)\n",
        "        # steps, rewards = equalize_lengths(steps, rewards)\n",
        "\n",
        "        if mode == \"best\":\n",
        "            best = rewards.sum(axis=1).argmax()\n",
        "            mean = rewards[best]\n",
        "        elif mode == \"max\":\n",
        "            mean = np.max(rewards, axis=0)\n",
        "        else:\n",
        "            std = rewards.std(axis=0)\n",
        "            mean = rewards.mean(axis=0)\n",
        "            ax.fill_between(steps, mean + std, mean - std, alpha=0.1, color=color)\n",
        "        ax.plot(steps, mean, lw=2, label=f\"{algo_name}\", color=color)\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "        plt.legend()\n",
        "\n",
        "        save_dir += f\"{env_name}/\"\n",
        "\n",
        "        clean_env_name = env_name.split(\"-\")[0]\n",
        "        figure_name = f\"{prefix}{clean_env_name.lower()}_{mode}\"\n",
        "        title = f\"{clean_env_name} ({mode})\"\n",
        "        if suffix:\n",
        "            figure_name += f\"{suffix}\"\n",
        "        final_show(save_fig, True, save_dir, figure_name, \"timesteps\", \"rewards\", title)\n",
        "\n",
        "    def plot_histograms(\n",
        "        self,\n",
        "        rewards,\n",
        "        env_name,\n",
        "        suffix=\"\",\n",
        "        save_dir=\"./plots/\",\n",
        "        plot=True,\n",
        "        save_fig=True,\n",
        "    ):\n",
        "        plt.figure(figsize=(9, 6))\n",
        "\n",
        "        colors = [\"#09b542\", \"#008fd5\", \"#fc4f30\", \"#e5ae38\", \"#e5ae38\", \"#810f7c\"]\n",
        "        # colors = [\"#fc4f30\", \"#008fd5\", \"#e5ae38\"]\n",
        "\n",
        "        n_bars = len(rewards)\n",
        "        x = np.arange(len(list(rewards.values())[0]))\n",
        "        width = 0.75 / n_bars\n",
        "\n",
        "        for i, reward in enumerate(rewards.values()):\n",
        "            plt.bar(x + width * i, np.sort(reward)[::-1], width=width, color=colors[i])\n",
        "\n",
        "        plt.legend(labels=rewards.keys())\n",
        "        plt.xticks([], [])\n",
        "\n",
        "        save_dir += f\"{env_name}/\"\n",
        "\n",
        "        clean_env_name = env_name.split(\"-\")[0]\n",
        "        title = clean_env_name\n",
        "        figure_name = f\"{clean_env_name.lower()}-histograms\"\n",
        "\n",
        "        if suffix:\n",
        "            title += f\" ({suffix})\"\n",
        "            figure_name += f\"{suffix}\"\n",
        "\n",
        "        final_show(save_fig, plot, save_dir, figure_name, \"\", \"rewards\", title)\n",
        "\n",
        "class RewardLogger:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "        self.episode = 0\n",
        "        self.all_rewards = []\n",
        "        self.all_rewards.append([])\n",
        "        self.all_steps = []\n",
        "\n",
        "    def add(self, nb_steps, reward):\n",
        "        if self.episode == 0:\n",
        "            self.all_steps.append(nb_steps)\n",
        "        self.all_rewards[self.episode].append(reward.item())\n",
        "\n",
        "    def new_episode(self):\n",
        "        self.episode += 1\n",
        "        self.all_rewards.append([])\n",
        "\n",
        "    def save(self):\n",
        "        # print(\"reward loader save:\", self.all_steps,  self.all_rewards)\n",
        "        with open(self.steps_filename, \"ab\") as f:\n",
        "            np.save(f, self.all_steps)\n",
        "        with open(self.rewards_filename, \"ab\") as f:\n",
        "            np.save(f, self.all_rewards)\n",
        "\n",
        "class RewardLoader:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "\n",
        "    def load(self):\n",
        "        with open(self.steps_filename, \"rb\") as f:\n",
        "            steps = np.load(f, allow_pickle=True)\n",
        "        with open(self.rewards_filename, \"rb\") as f:\n",
        "            rewards = np.load(f, allow_pickle=True)\n",
        "        return steps, rewards"
      ],
      "metadata": {
        "id": "Uvi68vi0aE7d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class DiscreteQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = build_mlp(\n",
        "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        \n",
        "        if choose_action:\n",
        "            action = q_values.argmax(1)\n",
        "            self.set((\"action\", t), action)\n",
        "\n",
        "\n",
        "class ContinuousQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.is_q_function = True\n",
        "        self.model = build_mlp(\n",
        "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, detach_actions=False):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        action = self.get((\"action\", t))\n",
        "        if detach_actions:\n",
        "            action = action.detach()\n",
        "        osb_act = torch.cat((obs, action), dim=1)\n",
        "        q_value = self.model(osb_act)\n",
        "        self.set((\"q_value\", t), q_value)\n",
        "\n",
        "    def predict_value(self, obs, action):\n",
        "        obs_act = torch.cat((obs, action), dim=0)\n",
        "        q_value = self.model(obs_act)\n",
        "        return q_value\n",
        "\n",
        "\n",
        "def make_gym_env(env_name, env_kwargs):\n",
        "    env = MazeMDPContinuousWrapper(\n",
        "        gym.make(env_name, kwargs=env_kwargs)\n",
        "    )\n",
        "    return env\n",
        "\n",
        "\n",
        "class EGreedyActionSelector(Agent):\n",
        "    def __init__(self, epsilon):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        q_values = self.get((\"q_values\", t))\n",
        "        nb_actions = q_values.size()[1]\n",
        "        size = q_values.size()[0]\n",
        "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
        "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
        "        max_action = q_values.max(1)[1]\n",
        "        action = is_random * random_action + (1 - is_random) * max_action\n",
        "        action = action.long()\n",
        "        self.set((\"action\", t), action)"
      ],
      "metadata": {
        "id": "qV_L1P7H2ZdY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "1v6XjQd9zKmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PARAMS"
      ],
      "metadata": {
        "id": "kNMqVXUxg9pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params={\n",
        "  \"save_best\": False,\n",
        "  \"plot_agents\": True,\n",
        "  \n",
        "  \"logger\":{\n",
        "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "    \"log_dir\": \"./dqn_logs/\",\n",
        "    \"cache_size\": 10000,\n",
        "    \"every_n_seconds\": 5,\n",
        "    \"verbose\": False,    \n",
        "  },\n",
        "\n",
        "  \"algorithm\":{\n",
        "    \"seed\": 3,\n",
        "    \"nb_seeds\": 1,\n",
        "    \"buffer_size\": 1e6,\n",
        "    \"epsilon\": 0.1,                 # valeur pour epsilon-greedy\n",
        "    \"n_updates\": 50,                # nb d'update par Replay Buffer\n",
        "    \"n_steps\": 64,                  # nb max de step par épisode ?\n",
        "    \"n_envs\": 1,                   # nb d'environnement en simultané (nb d'épisodes ?)\n",
        "    \"nb_episodes\": 50,              # nb d'épisodes\n",
        "    \"nb_evals\": 10,                 # nb d'évaluation après train\n",
        "    \"eval_interval\": 50,            # intervalle (steps) entre évaluations\n",
        "    \"discount_factor\": 0.99,        # delta\n",
        "    \"target_critic_update\": 100,    # intervalle (steps) entre chaque maj de Q_target\n",
        "    \"batch_size\": 100,              # taille du batch Replay Buffer\n",
        "    \"learning_starts\": 5,           # ???\n",
        "\n",
        "    \"max_grad_norm\": 0.5,\n",
        "    \"architecture\":{\"hidden_size\": [128, 128]},\n",
        "  },\n",
        "\n",
        "  \"gym_env\":{\n",
        "    \"classname\": \"__main__.make_gym_env\",\n",
        "    \"env_name\": \"MazeMDP-v0\",\n",
        "    \"env_kwargs\": {\"width\": 5, \"height\": 5, \"ratio\": 0.2},\n",
        "  },\n",
        "\n",
        "  \"optimizer\":{\n",
        "    \"classname\": \"torch.optim.Adam\",\n",
        "    \"lr\": 2e-3,\n",
        "  }\n",
        "}\n",
        "\n",
        "config = OmegaConf.create(params)"
      ],
      "metadata": {
        "id": "wyiQEOfXzLz7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SET-UP"
      ],
      "metadata": {
        "id": "Ndj4MZ4-hARb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_env_agents(cfg):\n",
        "    train_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.nb_evals,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent\n",
        "\n",
        "\n",
        "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "\n",
        "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
        "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
        "    target_critic = copy.deepcopy(critic)\n",
        "\n",
        "    q_agent = TemporalAgent(critic)\n",
        "    target_q_agent = TemporalAgent(target_critic)\n",
        "    \n",
        "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
        "    ev_agent = Agents(eval_env_agent, critic)\n",
        "    # Get an agent that is executed on a complete workspace\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "    \n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "    return train_agent, eval_agent, q_agent, target_q_agent\n",
        "\n",
        "\n",
        "# Configure the optimizer\n",
        "def setup_optimizers(cfg, q_agent):\n",
        "    optimizer_args = get_arguments(cfg.optimizer)\n",
        "    parameters = q_agent.parameters()\n",
        "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
        "    # Compute temporal difference\n",
        "    max_q = target_q_values.max(-1)[0].detach()\n",
        "\n",
        "    target = (\n",
        "        reward[:-1].squeeze()\n",
        "        + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
        "    )\n",
        "\n",
        "    act = action[0].unsqueeze(-1)\n",
        "    qvals = torch.gather(q_values, dim=1, index=act).squeeze()\n",
        "\n",
        "    mse = nn.MSELoss()\n",
        "    critic_loss = mse(target, qvals)\n",
        "    return critic_loss"
      ],
      "metadata": {
        "id": "HauPdovyH5fG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN"
      ],
      "metadata": {
        "id": "B85LDE9lhEVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn(cfg, reward_logger):\n",
        "    print(\"0\")\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg)\n",
        "    best_reward = -10e9\n",
        "    print(\"1\")\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
        "    print(\"2\")\n",
        "\n",
        "    # 3) Create the DQN-like Agent\n",
        "    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent)\n",
        "    print(\"3.1\")\n",
        "\n",
        "    # Used for training\n",
        "    train_workspace = Workspace()\n",
        "    print(\"3.2\")\n",
        "\n",
        "    # 4) Create the Replay Buffer Agent\n",
        "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
        "    print(\"4\")\n",
        "\n",
        "    # 5) Create the HER Agent\n",
        "    # her = HERAgent(pass)\n",
        "    # print(\"A\")\n",
        "\n",
        "    # 6) Configure the optimizer\n",
        "    optimizer = setup_optimizers(cfg, q_agent)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "    tmp_steps2 = 0\n",
        "    print(\"6\")\n",
        "\n",
        "    # 7) Training\n",
        "    # Train des épisodes        \n",
        "    train_agent(train_workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n",
        "    print(\"7.0.1\")\n",
        "\n",
        "    transition_workspace = train_workspace.get_transitions()\n",
        "    print(\"7.0.2\")\n",
        "\n",
        "    # comptage du nb de step de l'épisode\n",
        "    action = transition_workspace[\"action\"]\n",
        "    nb_steps += action[0].shape[0]\n",
        "    print(\"7.0.3\")\n",
        "\n",
        "    # ajout des transitions au RB\n",
        "    rb.put(transition_workspace)\n",
        "    print(\"7.0.4\")\n",
        "\n",
        "    # 7.1) Loop Replay Buffer\n",
        "    for _ in range(cfg.algorithm.n_updates):\n",
        "        # mélange et création du RB_W\n",
        "        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
        "\n",
        "        # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace).\n",
        "        q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
        "\n",
        "        q_values, done, truncated, reward, action = rb_workspace[\n",
        "            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
        "        ]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
        "\n",
        "        target_q_values = rb_workspace[\"q_values\"]\n",
        "        # assert torch.equal(q_values, target_q_values), \"values differ\"\n",
        "\n",
        "        # Determines whether values of the critic should be propagated\n",
        "        # True if the episode reached a time limit or if the task was not done\n",
        "        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
        "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "        if rb.size() > cfg.algorithm.learning_starts:\n",
        "            # Compute critic loss\n",
        "            critic_loss = compute_critic_loss(\n",
        "                cfg, reward, must_bootstrap, q_values[0], target_q_values[1], action\n",
        "            )\n",
        "\n",
        "            # Store the loss for tensorboard display\n",
        "            logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                q_agent.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            optimizer.step()\n",
        "    print(\"7.1\")\n",
        "\n",
        "    # 7.2) Maj du Q_target (sous conditions)\n",
        "    if nb_steps - tmp_steps2 > cfg.algorithm.target_critic_update:\n",
        "        tmp_steps2 = nb_steps\n",
        "        target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
        "        print(\"7.2\")\n",
        "\n",
        "    # 7.3) Évaluation régulère\n",
        "    if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "        tmp_steps = nb_steps\n",
        "        eval_workspace = Workspace()  # Used for evaluation\n",
        "        eval_agent(\n",
        "            eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
        "        )\n",
        "        rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "        mean = rewards.mean()\n",
        "        logger.add_log(\"reward\", mean, nb_steps)\n",
        "        reward_logger.add(nb_steps, mean)\n",
        "\n",
        "        if cfg.save_best and mean > best_reward:\n",
        "            best_reward = mean\n",
        "            directory = \"./dqn_critic/\"\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            filename = directory + \"dqn_\" + str(mean.item()) + \".agt\"\n",
        "            eval_agent.save_model(filename)\n",
        "\n",
        "            if cfg.plot_agents:\n",
        "                policy = eval_agent.agent.agents[1]\n",
        "                plot_policy(\n",
        "                    policy,\n",
        "                    eval_env_agent,\n",
        "                    \"./dqn_plots/\",\n",
        "                    cfg.gym_env.env_name,\n",
        "                    best_reward,\n",
        "                    stochastic=False,\n",
        "                )\n",
        "                plot_critic(\n",
        "                    policy,\n",
        "                    eval_env_agent,\n",
        "                    \"./dqn_plots/\",\n",
        "                    cfg.gym_env.env_name,\n",
        "                    best_reward,\n",
        "                )\n",
        "        print(\"7.3\")"
      ],
      "metadata": {
        "id": "ntdxI1XDR2CP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN"
      ],
      "metadata": {
        "id": "u18OyRDdqMBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_loop(cfg):\n",
        "    chrono = Chrono()\n",
        "    logdir = \"./plot/\"\n",
        "\n",
        "    if not os.path.exists(logdir):\n",
        "        os.makedirs(logdir)\n",
        "        \n",
        "    reward_logger = RewardLogger(logdir + \"dqn_her.steps\", logdir + \"dqn_her.rwd\")\n",
        "\n",
        "    for seed in range(cfg.algorithm.nb_seeds):\n",
        "        cfg.algorithm.seed = seed\n",
        "        torch.manual_seed(cfg.algorithm.seed)\n",
        "        run_dqn(cfg, reward_logger)\n",
        "        if seed < cfg.algorithm.nb_seeds - 1:\n",
        "            reward_logger.new_episode()\n",
        "\n",
        "    reward_logger.save()\n",
        "    chrono.stop()\n",
        "    plotter = Plotter(logdir + \"dqn_her.steps\", logdir + \"dqn_her.rwd\")\n",
        "    plotter.plot_reward(\"dqn her\", cfg.gym_env.env_name)"
      ],
      "metadata": {
        "id": "GSN9tztCZX-7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_loop(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "jfewpf6pZgQ2",
        "outputId": "ca78a960-d018-4295-d582-c2b7370100bd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3.1\n",
            "3.2\n",
            "4\n",
            "6\n",
            "7.0.1\n",
            "7.0.2\n",
            "7.0.3\n",
            "7.0.4\n",
            "7.1\n",
            "Time : 0s 678ms\n",
            "[2500] [[0.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}