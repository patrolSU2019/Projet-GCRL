{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install importlib-metadata==4.13.0\n",
    "!pip install git+https://github.com/osigaud/bbrl\n",
    "!pip install git+https://github.com/osigaud/bbrl_examples.git\n",
    "!pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.workspace import Workspace\n",
    "from bbrl import get_class, get_arguments, instantiate_class\n",
    "\n",
    "#import bbrl_gym\n",
    "import gym\n",
    "\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl.agents import Agents, TemporalAgent, PrintAgent\n",
    "from bbrl.agents.gymb import NoAutoResetGymAgent\n",
    "\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "from bbrl.utils.chrono import Chrono\n",
    "\n",
    "from bbrl.visu.visu_policies import plot_policy\n",
    "from bbrl.visu.visu_critics import plot_critic\n",
    "from bbrl.visu.common import final_show\n",
    "\n",
    "from bbrl_examples.models.loggers import RewardLoader\n",
    "from bbrl_examples.models.loggers import RewardLogger\n",
    "from bbrl_examples.models.loggers import Logger\n",
    "from bbrl_examples.models.plotters import Plotter\n",
    "from bbrl_examples.models.shared_models import build_mlp, build_alt_mlp\n",
    "from bbrl_examples.models.critics import DiscreteQAgent\n",
    "from bbrl_examples.models.exploration_agents import EGreedyActionSelector\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import os\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, goal_position, goal_angle, threshold=0.1):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.goal_position = goal_position\n",
    "        self.goal_angle = goal_angle\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def compute_distance(self,state):\n",
    "        position,angle= state[0],state[2]\n",
    "        return np.sqrt((position-self.goal_position)**2+(angle-self.goal_angle)**2)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        distance = self.compute_distance(state)\n",
    "\n",
    "        if distance < self.threshold:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        return state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def forward(self, t, achieved_goal, **kwargs):\n",
    "        # Modification du but\n",
    "        self.set((\"env/desired_goal\", t), achieved_goal)\n",
    "   \n",
    "\n",
    "    def set_goal(self, t, achieved_goal):\n",
    "        goal_position, goal_angle = achieved_goal\n",
    "        self.env.goal_position = goal_position\n",
    "        self.env.goal_angle = goal_angle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HERAgent(Agent):\n",
    "    def __init__(self,strategy= 'final'):\n",
    "        super().__init__()\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def forward(self, t, transitions):\n",
    "        her_transitions = Workspace()\n",
    "        if self.strategy == 'final':\n",
    "            achieved_goal = transitions[\"env/env_obs\"][-1]\n",
    "        elif self.strategy == 'future':\n",
    "            # Select a random future transition\n",
    "            index = random.randint(0, len(transitions[\"env/env_obs\"]) - 1)\n",
    "            achieved_goal = transitions[\"env/env_obs\"][index]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HER strategy\")\n",
    "\n",
    "        # Replace the desired_goal with the achieved_goal\n",
    "      \n",
    "        her_transitions.set((\"env/achieved_goal\", t), achieved_goal)\n",
    "        return her_transitions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalRelabellingAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t, achieved_goal, **kwargs):\n",
    "        # modification du but\n",
    "        self.set((\"env/desired_goal\", t), achieved_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardAgent(Agent):\n",
    "    def __init__(self,reward_threshold = 0.1):\n",
    "        super().__init__()\n",
    "        self.reward_threshold = reward_threshold\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        if t != 0:\n",
    "            desired_goal = self.get((\"env/desired_goal\", t))\n",
    "            achieved_goal = self.get((\"env/achieved_goal\", t))\n",
    "        \n",
    "            distance = np.linalg.norm(desired_goal - achieved_goal)\n",
    "\n",
    "            if distance < self.reward_threshold:\n",
    "                reward = 1\n",
    "                done = True\n",
    "            else:\n",
    "                reward = 0\n",
    "                done = False\n",
    "            self.set((\"env/reward\", t), reward)\n",
    "            self.set((\"env/done\", t), done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCDQNAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_alt_mlp(\n",
    "            [state_dim +4] + list(hidden_layers) + [action_dim], activation=nn.ReLU()  )\n",
    "\n",
    "    def forward(self, t, choose_action=True, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        goal = self.get((\"env/achieved_goal\", t))\n",
    "        agent_input = torch.cat([obs, goal], dim=1)\n",
    "\n",
    "        q_values = self.model(agent_input).squeeze(-1)\n",
    "        self.set((\"q_values\", t), q_values)\n",
    "\n",
    "        if choose_action:\n",
    "            action = q_values.argmax(-1)\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "    def predict_action(self, obs, goal, stochastic):\n",
    "        agent_input = torch.cat([obs, goal], dim=1)\n",
    "        q_values = self.model(obs).squeeze(-1)\n",
    "        if stochastic:\n",
    "            probs = torch.softmax(q_values, dim=-1)\n",
    "            action = torch.distributions.Categorical(probs).sample()\n",
    "        else:\n",
    "            action = q_values.argmax(-1)\n",
    "        return action\n",
    "\n",
    "    def predict_value(self, obs, goal, action):\n",
    "        agent_input = torch.cat([obs, goal], dim=1)\n",
    "        q_values = self.model(agent_input).squeeze(-1)\n",
    "        return q_values[action[0].int()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gym_env(env_name):\n",
    "    #env = CartWrapper(gym.make(env_name))\n",
    "    #return env\n",
    "    return gym.make(env_name)\n",
    "\n",
    "\n",
    "def get_env_agents(cfg):\n",
    "    train_env_agent = NoAutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.n_envs,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "    # print_agent = PrintAgent()\n",
    "    eval_env_agent = NoAutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.nb_evals,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "    return train_env_agent, eval_env_agent\n",
    "\n",
    "\n",
    "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "\n",
    "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
    "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "\n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "    #her_agent = HERAgent()\n",
    "\n",
    "    #tr_agent = Agents(train_env_agent, critic, explorer, her_agent)\n",
    "    #ev_agent = Agents(eval_env_agent, critic, her_agent)\n",
    "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
    "    ev_agent = Agents(eval_env_agent, critic)\n",
    "    \n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "\n",
    "    goal_label_agent = TemporalAgent(GoalRelabellingAgent())\n",
    "    reward_agent = TemporalAgent(RewardAgent())\n",
    "\n",
    "    train_agent.seed(cfg.algorithm.seed)\n",
    "    #return train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, reward_agent, her_agent\n",
    "    return train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, reward_agent\n",
    "\n",
    "\n",
    "# Configure the optimizer\n",
    "def setup_optimizers(cfg, q_agent):\n",
    "    optimizer_args = get_arguments(cfg.optimizer)\n",
    "    parameters = q_agent.parameters()\n",
    "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
    "    # Compute temporal difference\n",
    "    max_q = target_q_values[1].max(-1)[0].detach()\n",
    "\n",
    "    target = (\n",
    "        reward[:-1]\n",
    "        + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
    "    )\n",
    "\n",
    "    vals = q_values.squeeze()\n",
    "    qvals = torch.gather(vals, dim=1, index=action)\n",
    "    qvals = qvals[:-1]\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, qvals)\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(cfg, reward_logger):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = -10e9\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    # 3) Create the DQN-like Agent\n",
    "    #train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, reward_agent,her_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent)\n",
    "    train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, reward_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent)\n",
    "\n",
    "    # 4) Create the Replay Buffer Agent\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # 6) Configure the optimizer\n",
    "    optimizer = setup_optimizers(cfg, q_agent)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "    tmp_steps2 = 0\n",
    "\n",
    "    # 7) Training\n",
    "    # Train des épisodes\n",
    "    for _ in range(cfg.algorithm.n_episodes):\n",
    "        train_workspace = Workspace()\n",
    "        train_agent(train_workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        \n",
    "        # comptage du nb de step de l'épisode\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "\n",
    "        # ajout des transitions au RB\n",
    "        rb.put(transition_workspace)\n",
    "\n",
    "        # 7.1) Loop Replay Buffer\n",
    "        for _ in range(cfg.algorithm.n_updates):\n",
    "            # tirage aléatoire d'un minibatch dans un Workspace\n",
    "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "            achieved_goal = rb_workspace[\"env/env_obs\"][-1]\n",
    "            #goal_label_agent(rb_workspace, t=0, achieved_goal=achieved_goal, n_steps=2, choose_action=False)\n",
    "            #reward_agent(rb_workspace, t=0, n_steps=2)\n",
    "\n",
    "            # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace).\n",
    "            q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "\n",
    "            q_values, done, truncated, reward, action = rb_workspace[\n",
    "                \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
    "            ]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "\n",
    "            target_q_values = rb_workspace[\"q_values\"]\n",
    "            # assert torch.equal(q_values, target_q_values), \"values differ\"\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
    "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
    "\n",
    "            if rb.size() > cfg.algorithm.learning_starts:\n",
    "                # Compute critic loss\n",
    "                critic_loss = compute_critic_loss(\n",
    "                    cfg, reward, must_bootstrap, q_values[0], target_q_values[1], action\n",
    "                )\n",
    "\n",
    "                # Store the loss for tensorboard display\n",
    "                logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    q_agent.parameters(), cfg.algorithm.max_grad_norm\n",
    "                )\n",
    "                optimizer.step()\n",
    "\n",
    "        # 7.2) Maj du Q_target (sous conditions)\n",
    "        if nb_steps - tmp_steps2 > cfg.algorithm.target_critic_update:\n",
    "            tmp_steps2 = nb_steps\n",
    "            target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
    "        \n",
    "        # 7.3) Évaluation régulère\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.add_log(\"reward\", mean, nb_steps)\n",
    "            print(f\"reward: {mean}\")\n",
    "            reward_logger.add(nb_steps, mean)\n",
    "            \n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = \"./dqn_critic/\"\n",
    "                \n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                \n",
    "                filename = directory + \"dqn_\" + str(mean.item()) + \".agt\"\n",
    "                eval_agent.save_model(filename)\n",
    "                \n",
    "                if cfg.plot_agents:\n",
    "                    policy = eval_agent.agent.agents[1]\n",
    "                    plot_policy(\n",
    "                        policy,\n",
    "                        eval_env_agent,\n",
    "                        \"./dqn_plots/\",\n",
    "                        cfg.gym_env.env_name,\n",
    "                        best_reward,\n",
    "                        stochastic=False,\n",
    "                    )\n",
    "                    plot_critic(\n",
    "                        policy,\n",
    "                        eval_env_agent,\n",
    "                        \"./dqn_plots/\",\n",
    "                        cfg.gym_env.env_name,\n",
    "                        best_reward,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"plot_agents\": True,\n",
    "  \n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./dqn_logs/\",\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 1,\n",
    "    \"verbose\": False,    \n",
    "  },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 3,                      # modifié par la main loop\n",
    "    \"nb_seeds\": 1,                  # nb de seed testées (de 0 à valeur proposée)\n",
    "\n",
    "    \"epsilon\": 0.05,                 # valeur pour epsilon-greedy\n",
    "    \"discount_factor\": 0.99,        # delta\n",
    "    \"gae\": 0.8,                     # ???\n",
    "\n",
    "    \"n_steps\": 64,                  # nb max de step par épisode ?\n",
    "    \"n_envs\": 10,                    # nb d'environnement en simultané\n",
    "    \"n_episodes\": 20,                # nb d'épisodes\n",
    "    \"nb_evals\": 5,                 # nb d'évaluation après train\n",
    "    \"eval_interval\": 10,            # intervalle (steps) entre évaluations ?\n",
    "    \"target_critic_update\": 10,    # intervalle (steps) entre chaque maj de Q_target ?\n",
    "\n",
    "    \"learning_starts\": 1,           # ???\n",
    "\n",
    "    \"n_updates\": 20,                # nb d'update par le Replay Buffer\n",
    "    \"buffer_size\": 1e6,             # taille max du Replay Buffer\n",
    "    \"batch_size\": 50,              # taille du batch Replay Buffer\n",
    "\n",
    "    \"max_grad_norm\": 0.5,           # ???\n",
    "    \"architecture\":{\"hidden_size\": [128, 128]},\n",
    "  },\n",
    "\n",
    "  \"gym_env\":{\n",
    "    \"classname\": \"__main__.make_gym_env\",\n",
    "    \"env_name\": \"CartPole-v1\"\n",
    "  },\n",
    "\n",
    "  \"optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 2.3e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "config = OmegaConf.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(cfg):\n",
    "    chrono = Chrono()\n",
    "    logdir = \"./plot/\"\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    reward_logger = RewardLogger(\n",
    "        logdir + \"dqn.steps\", logdir + \"dqn.rwd\"\n",
    "    )\n",
    "\n",
    "    for seed in range(cfg.algorithm.nb_seeds):\n",
    "        #cfg.algorithm.seed = seed\n",
    "        torch.manual_seed(cfg.algorithm.seed)\n",
    "        run_dqn(cfg, reward_logger)\n",
    "\n",
    "        if seed < cfg.algorithm.nb_seeds - 1:\n",
    "            reward_logger.new_episode()\n",
    "\n",
    "    reward_logger.save()\n",
    "    chrono.stop()\n",
    "    plotter = Plotter(logdir + \"dqn.steps\", logdir + \"dqn.rwd\")\n",
    "    plotter.plot_reward(\"dqn\", cfg.gym_env.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 9.399999618530273\n",
      "reward: 9.399999618530273\n",
      "reward: 9.199999809265137\n",
      "reward: 9.199999809265137\n",
      "reward: 9.399999618530273\n",
      "reward: 9.800000190734863\n",
      "reward: 9.0\n",
      "reward: 8.800000190734863\n",
      "reward: 10.0\n",
      "reward: 10.199999809265137\n",
      "reward: 9.199999809265137\n",
      "reward: 9.800000190734863\n",
      "reward: 9.600000381469727\n",
      "reward: 9.600000381469727\n",
      "reward: 9.399999618530273\n",
      "reward: 9.399999618530273\n",
      "reward: 10.0\n",
      "reward: 9.0\n",
      "reward: 9.0\n",
      "reward: 9.399999618530273\n",
      "Time : 5s 801ms\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "main_loop(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
