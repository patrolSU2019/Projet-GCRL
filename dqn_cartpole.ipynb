{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pVXOYNmls5Uv",
        "LZXcFYSBs550",
        "hMJF8ZpbyLt0",
        "1WTM15ZG3QcF",
        "kNMqVXUxg9pQ",
        "Ndj4MZ4-hARb",
        "B85LDE9lhEVj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations/Préparations"
      ],
      "metadata": {
        "id": "pVXOYNmls5Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install"
      ],
      "metadata": {
        "id": "LZXcFYSBs550"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jMZrJSJHsd0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3522ed39-a67b-4f45-8543-1647b37526fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.9/dist-packages (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata==4.13.0) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl\n",
            "  Cloning https://github.com/osigaud/bbrl to /tmp/pip-req-build-szkkqwpy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl /tmp/pip-req-build-szkkqwpy\n",
            "  Resolved https://github.com/osigaud/bbrl to commit cb2c22b82bfedfb04d8232ba432cdbd798fd797a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.9/dist-packages (2.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from omegaconf) (6.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.9/dist-packages (from omegaconf) (4.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install importlib-metadata==4.13.0\n",
        "#!pip install setuptools==65.5.0\n",
        "#!pip install git+https://github.com/osigaud/bbrl_gym\n",
        "!pip install git+https://github.com/osigaud/bbrl\n",
        "!pip install omegaconf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "hMJF8ZpbyLt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bbrl.workspace import Workspace\n",
        "from bbrl import get_class, get_arguments, instantiate_class\n",
        "\n",
        "#import bbrl_gym\n",
        "import gym\n",
        "\n",
        "from bbrl.agents.agent import Agent\n",
        "from bbrl.agents import Agents, TemporalAgent, PrintAgent\n",
        "from bbrl.agents.gymb import NoAutoResetGymAgent\n",
        "\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "from bbrl.utils.chrono import Chrono\n",
        "\n",
        "from bbrl.visu.visu_policies import plot_policy\n",
        "from bbrl.visu.visu_critics import plot_critic\n",
        "from bbrl.visu.common import final_show\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from omegaconf import DictConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib\n",
        "import os\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import copy"
      ],
      "metadata": {
        "id": "P5EkdUXfslay"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implémentations depuis bbrl_examples"
      ],
      "metadata": {
        "id": "1WTM15ZG3QcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "    def __init__(self, cfg):\n",
        "        self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "    def add_log(self, log_string, loss, epoch):\n",
        "        self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "    # Log losses\n",
        "    def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n",
        "        self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "        self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "        self.add_log(\"actor_loss\", actor_loss, epoch)\n",
        "\n",
        "    def log_reward_losses(self, rewards, nb_steps):\n",
        "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
        "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
        "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
        "        self.add_log(\"reward/median\", rewards.median(), nb_steps)\n",
        "\n",
        "def format_num(num, pos):\n",
        "    # Pos is a required parameter, but it is not used\n",
        "    magnitude = 0\n",
        "    labels = [\"\", \"K\", \"M\", \"G\"]\n",
        "    while abs(num) >= 1e3:\n",
        "        magnitude += 1\n",
        "        num /= 1e3\n",
        "\n",
        "    return f\"{num:.1f}{labels[magnitude]}\"\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "\n",
        "    def plot_reward(\n",
        "        self,\n",
        "        algo_name,\n",
        "        env_name,\n",
        "        mode=\"mean\",\n",
        "        prefix=\"\",\n",
        "        suffix=\".pdf\",\n",
        "        save_fig=True,\n",
        "        save_dir=\"./plots/\",\n",
        "    ):\n",
        "        _, ax = plt.subplots(figsize=(9, 6))\n",
        "        formatter = FuncFormatter(format_num)\n",
        "\n",
        "        colors = [\"#09b542\", \"#008fd5\", \"#fc4f30\", \"#e5ae38\", \"#e5ae38\", \"#810f7c\"]\n",
        "        color = colors[0]\n",
        "\n",
        "        loader = RewardLoader(self.steps_filename, self.rewards_filename)\n",
        "        steps, rewards = loader.load()\n",
        "        print(steps, rewards)\n",
        "        # steps, rewards = equalize_lengths(steps, rewards)\n",
        "\n",
        "        if mode == \"best\":\n",
        "            best = rewards.sum(axis=1).argmax()\n",
        "            mean = rewards[best]\n",
        "        elif mode == \"max\":\n",
        "            mean = np.max(rewards, axis=0)\n",
        "        else:\n",
        "            std = rewards.std(axis=0)\n",
        "            mean = rewards.mean(axis=0)\n",
        "            ax.fill_between(steps, mean + std, mean - std, alpha=0.1, color=color)\n",
        "        ax.plot(steps, mean, lw=2, label=f\"{algo_name}\", color=color)\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "        plt.legend()\n",
        "\n",
        "        save_dir += f\"{env_name}/\"\n",
        "\n",
        "        clean_env_name = env_name.split(\"-\")[0]\n",
        "        figure_name = f\"{prefix}{clean_env_name.lower()}_{mode}\"\n",
        "        title = f\"{clean_env_name} ({mode})\"\n",
        "        if suffix:\n",
        "            figure_name += f\"{suffix}\"\n",
        "        final_show(save_fig, True, save_dir, figure_name, \"timesteps\", \"rewards\", title)\n",
        "\n",
        "    def plot_histograms(\n",
        "        self,\n",
        "        rewards,\n",
        "        env_name,\n",
        "        suffix=\"\",\n",
        "        save_dir=\"./plots/\",\n",
        "        plot=True,\n",
        "        save_fig=True,\n",
        "    ):\n",
        "        plt.figure(figsize=(9, 6))\n",
        "\n",
        "        colors = [\"#09b542\", \"#008fd5\", \"#fc4f30\", \"#e5ae38\", \"#e5ae38\", \"#810f7c\"]\n",
        "        # colors = [\"#fc4f30\", \"#008fd5\", \"#e5ae38\"]\n",
        "\n",
        "        n_bars = len(rewards)\n",
        "        x = np.arange(len(list(rewards.values())[0]))\n",
        "        width = 0.75 / n_bars\n",
        "\n",
        "        for i, reward in enumerate(rewards.values()):\n",
        "            plt.bar(x + width * i, np.sort(reward)[::-1], width=width, color=colors[i])\n",
        "\n",
        "        plt.legend(labels=rewards.keys())\n",
        "        plt.xticks([], [])\n",
        "\n",
        "        save_dir += f\"{env_name}/\"\n",
        "\n",
        "        clean_env_name = env_name.split(\"-\")[0]\n",
        "        title = clean_env_name\n",
        "        figure_name = f\"{clean_env_name.lower()}-histograms\"\n",
        "\n",
        "        if suffix:\n",
        "            title += f\" ({suffix})\"\n",
        "            figure_name += f\"{suffix}\"\n",
        "\n",
        "        final_show(save_fig, plot, save_dir, figure_name, \"\", \"rewards\", title)\n",
        "\n",
        "class RewardLogger:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "        self.episode = 0\n",
        "        self.all_rewards = []\n",
        "        self.all_rewards.append([])\n",
        "        self.all_steps = []\n",
        "\n",
        "    def add(self, nb_steps, reward):\n",
        "        if self.episode == 0:\n",
        "            self.all_steps.append(nb_steps)\n",
        "        self.all_rewards[self.episode].append(reward.item())\n",
        "\n",
        "    def new_episode(self):\n",
        "        self.episode += 1\n",
        "        self.all_rewards.append([])\n",
        "\n",
        "    def save(self):\n",
        "        # print(\"reward loader save:\", self.all_steps,  self.all_rewards)\n",
        "        with open(self.steps_filename, \"ab\") as f:\n",
        "            np.save(f, self.all_steps)\n",
        "        with open(self.rewards_filename, \"ab\") as f:\n",
        "            np.save(f, self.all_rewards)\n",
        "\n",
        "class RewardLoader:\n",
        "    def __init__(self, steps_filename, rewards_filename):\n",
        "        self.steps_filename = steps_filename\n",
        "        self.rewards_filename = rewards_filename\n",
        "\n",
        "    def load(self):\n",
        "        with open(self.steps_filename, \"rb\") as f:\n",
        "            steps = np.load(f, allow_pickle=True)\n",
        "        with open(self.rewards_filename, \"rb\") as f:\n",
        "            rewards = np.load(f, allow_pickle=True)\n",
        "        return steps, rewards"
      ],
      "metadata": {
        "id": "Uvi68vi0aE7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1060db23-a6c0-4e81-b6de-ef7a8dffbd27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def build_alt_mlp(sizes, activation):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        if j < len(sizes) - 2:\n",
        "            layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n",
        "        else:\n",
        "            layers += [nn.Linear(sizes[j], sizes[j + 1])]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class DiscreteQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.is_q_function = True\n",
        "        self.model = build_alt_mlp(\n",
        "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(-1)\n",
        "            self.set((\"action\", t), action)\n",
        "\n",
        "    def predict_action(self, obs, stochastic):\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        if stochastic:\n",
        "            probs = torch.softmax(q_values, dim=-1)\n",
        "            action = torch.distributions.Categorical(probs).sample()\n",
        "        else:\n",
        "            action = q_values.argmax(-1)\n",
        "        return action\n",
        "\n",
        "    def predict_value(self, obs, action):\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        return q_values[action[0].int()]\n",
        "\n",
        "\n",
        "class EGreedyActionSelector(Agent):\n",
        "    def __init__(self, epsilon):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        q_values = self.get((\"q_values\", t))\n",
        "        nb_actions = q_values.size()[1]\n",
        "        size = q_values.size()[0]\n",
        "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
        "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
        "        max_action = q_values.max(1)[1]\n",
        "        action = is_random * random_action + (1 - is_random) * max_action\n",
        "        action = action.long()\n",
        "        self.set((\"action\", t), action)"
      ],
      "metadata": {
        "id": "qV_L1P7H2ZdY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouveaux Agents & Wrapper"
      ],
      "metadata": {
        "id": "ppvuEIU13vFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vraiment utile ? Wrapper/env inatégnables dans le run\n",
        "# Non utilisée pour l'instant\n",
        "class CartWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CartWrapper, self).__init__(env)\n",
        "\n",
        "        # choix arbitraire d'un goal de départ\n",
        "        pos = random.uniform(-2.4, 2.4)\n",
        "        vel = random.uniform(-1e3, 1e3)\n",
        "        angle = random.uniform(-.2095, .2095)\n",
        "        angle_vel = random.uniform(-1e3, 1e3)\n",
        "        # set du paramètre goal de l'environnement\n",
        "        self.goal = (pos, vel, angle, angle_vel)\n",
        "\n",
        "        # distance acceptable autour du but\n",
        "        self.eps = 0.5\n",
        "\n",
        "    def step(self, action):\n",
        "        # effectue un step de l'env CartPole\n",
        "        next_state, _, _, info = self.env.step(action)\n",
        "\n",
        "        # calcul de distance entre next_state et le but\n",
        "        distance = np.linalg.norm(next_state - self.goal)\n",
        "\n",
        "        # récompense et fin si distance inférieure à eps\n",
        "        if distance < self.eps:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "\n",
        "class GoalRelabellingAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, t, desired_goal=None, achieved_goal=None, choice=False, **kwargs):\n",
        "        if desired_goal != None:\n",
        "            # modification du but désiré\n",
        "            self.set((\"env/desired_goal\", t), desired_goal)\n",
        "\n",
        "        if achieved_goal != None:\n",
        "            # modification du but atteind\n",
        "            self.set((\"env/achieved_goal\", t), achieved_goal)\n",
        "\n",
        "        if choice:\n",
        "            # choix arbitraire d'un goal\n",
        "            pos = random.uniform(-2.4, 2.4)\n",
        "            vel = random.uniform(-1e3, 1e3)\n",
        "            angle = random.uniform(-.2095, .2095)\n",
        "            angle_vel = random.uniform(-1e3, 1e3)\n",
        "\n",
        "            # set du paramètre goal de l'environnement\n",
        "            goal = np.array([pos, vel, angle, angle_vel])\n",
        "            ### TypeError: 'int' object is not callable ?\n",
        "            self.set((\"env/desired_goal\", t), goal)\n",
        "\n",
        "\n",
        "class RewardAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        if t != 0:\n",
        "            # récupération des buts atteints et désirés\n",
        "            desired_goal = self.get((\"env/desired_goal\", t))\n",
        "            obs = self.get((\"env/env_obs\", t))\n",
        "            eps = [0.5]*len(obs)\n",
        "\n",
        "            # calcul de distance entre next_state et le but\n",
        "            distance = np.linalg.norm(desired_goal - obs, axis=1)\n",
        "\n",
        "            # récompense et fin si distance inférieure à eps\n",
        "            reward = np.where(distance < eps, 1., 0.)\n",
        "            done = (distance < eps)\n",
        "\n",
        "            # set de la reward et du done\n",
        "            ### TypeError: 'int' object is not callable ?\n",
        "            self.set((\"env/reward\", t), reward)\n",
        "            self.set((\"env/done\", t), done)\n",
        "\n",
        "\n",
        "# En cours\n",
        "class HERAgentFinal(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, t, rb, **kwargs):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GCDQNAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.is_q_function = True\n",
        "        self.model = build_alt_mlp(\n",
        "            [state_dim +4] + list(hidden_layers) + [action_dim], activation=nn.ReLU()   # +4 car taille d'un état ?\n",
        "        )\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        # récupération des valeurs pour input\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        goal = self.get((\"env/desired_goal\", t))\n",
        "        agent_input = torch.cat([obs, goal], dim=1)\n",
        "\n",
        "        # calcul des valeurs\n",
        "        q_values = self.model(agent_input).squeeze(-1)\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(-1)\n",
        "            self.set((\"action\", t), action)\n",
        "\n",
        "    def predict_action(self, obs, stochastic):\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        if stochastic:\n",
        "            probs = torch.softmax(q_values, dim=-1)\n",
        "            action = torch.distributions.Categorical(probs).sample()\n",
        "        else:\n",
        "            action = q_values.argmax(-1)\n",
        "        return action\n",
        "\n",
        "    def predict_value(self, obs, action):\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        return q_values[action[0].int()]"
      ],
      "metadata": {
        "id": "PpPmz_8R97pf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-Up"
      ],
      "metadata": {
        "id": "1v6XjQd9zKmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paramètres"
      ],
      "metadata": {
        "id": "kNMqVXUxg9pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params={\n",
        "  \"save_best\": False,\n",
        "  \"plot_agents\": True,\n",
        "  \n",
        "  \"logger\":{\n",
        "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "    \"log_dir\": \"./dqn_logs/\",\n",
        "    \"cache_size\": 10000,\n",
        "    \"every_n_seconds\": 1,\n",
        "    \"verbose\": False,    \n",
        "  },\n",
        "\n",
        "  \"algorithm\":{\n",
        "    \"seed\": 3,                      # modifié par la main loop\n",
        "    \"nb_seeds\": 1,                  # nb de seed testées (de 0 à valeur proposée)\n",
        "\n",
        "    \"epsilon\": 0.05,                 # valeur pour epsilon-greedy\n",
        "    \"discount_factor\": 0.99,        # delta\n",
        "    \"gae\": 0.8,                     # ???\n",
        "\n",
        "    \"n_steps\": 64,                  # nb max de step par épisode ?\n",
        "    \"n_envs\": 10,                    # nb d'environnement en simultané\n",
        "    \"n_episodes\": 20,                # nb d'épisodes\n",
        "    \"nb_evals\": 5,                 # nb d'évaluation après train\n",
        "    \"eval_interval\": 10,            # intervalle (steps) entre évaluations ?\n",
        "    \"target_critic_update\": 10,    # intervalle (steps) entre chaque maj de Q_target ?\n",
        "\n",
        "    \"learning_starts\": 1,           # ???\n",
        "\n",
        "    \"n_updates\": 20,                # nb d'update par le Replay Buffer\n",
        "    \"buffer_size\": 1e6,             # taille max du Replay Buffer\n",
        "    \"batch_size\": 50,              # taille du batch Replay Buffer\n",
        "\n",
        "    \"max_grad_norm\": 0.5,           # ???\n",
        "    \"architecture\":{\"hidden_size\": [128, 128]},\n",
        "  },\n",
        "\n",
        "  \"gym_env\":{\n",
        "    \"classname\": \"__main__.make_gym_env\",\n",
        "    \"env_name\": \"CartPole-v1\"\n",
        "  },\n",
        "\n",
        "  \"optimizer\":{\n",
        "    \"classname\": \"torch.optim.Adam\",\n",
        "    \"lr\": 2.3e-3,\n",
        "  }\n",
        "}\n",
        "\n",
        "config = OmegaConf.create(params)"
      ],
      "metadata": {
        "id": "wyiQEOfXzLz7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "Ndj4MZ4-hARb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gym_env(env_name):\n",
        "    #env = CartWrapper(gym.make(env_name))\n",
        "    #return env\n",
        "    return gym.make(env_name)\n",
        "\n",
        "\n",
        "def get_env_agents(cfg):\n",
        "    train_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    # print_agent = PrintAgent()\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.nb_evals,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent\n",
        "\n",
        "\n",
        "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "\n",
        "    critic = GCDQNAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
        "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
        "    target_critic = copy.deepcopy(critic)\n",
        "\n",
        "    goal_label_agent = TemporalAgent(GoalRelabellingAgent())\n",
        "    reward_agent = RewardAgent()\n",
        "    her_agent = TemporalAgent(HERAgentFinal())\n",
        "\n",
        "    q_agent = TemporalAgent(critic)\n",
        "    target_q_agent = TemporalAgent(target_critic)\n",
        "    \n",
        "    tr_agent = Agents(train_env_agent, critic, explorer, reward_agent)\n",
        "    ev_agent = Agents(eval_env_agent, critic, reward_agent)\n",
        "\n",
        "    # Get an agent that is executed on a complete workspace\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "\n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "    return train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, her_agent\n",
        "\n",
        "\n",
        "# Configure the optimizer\n",
        "def setup_optimizers(cfg, q_agent):\n",
        "    optimizer_args = get_arguments(cfg.optimizer)\n",
        "    parameters = q_agent.parameters()\n",
        "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
        "    # Compute temporal difference\n",
        "    max_q = target_q_values[1].max(-1)[0].detach()\n",
        "\n",
        "    target = (\n",
        "        reward[:-1]\n",
        "        + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
        "    )\n",
        "\n",
        "    vals = q_values.squeeze()\n",
        "    qvals = torch.gather(vals, dim=1, index=action)\n",
        "    qvals = qvals[:-1]\n",
        "\n",
        "    mse = nn.MSELoss()\n",
        "    critic_loss = mse(target, qvals)\n",
        "    return critic_loss"
      ],
      "metadata": {
        "id": "HauPdovyH5fG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Méthode Run DQN"
      ],
      "metadata": {
        "id": "B85LDE9lhEVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn(cfg, reward_logger):\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg)\n",
        "    best_reward = -10e9\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
        "\n",
        "    # 3) Create the DQN-like Agent\n",
        "    #train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent)\n",
        "    train_agent, eval_agent, q_agent, target_q_agent, goal_label_agent, her_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent)\n",
        "\n",
        "    # 4) Create the Replay Buffer Agent\n",
        "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
        "\n",
        "    # 6) Configure the optimizer\n",
        "    optimizer = setup_optimizers(cfg, q_agent)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "    tmp_steps2 = 0\n",
        "\n",
        "    # 7) Training\n",
        "    # Train des épisodes\n",
        "    for _ in range(cfg.algorithm.n_episodes):\n",
        "        train_workspace = Workspace()\n",
        "\n",
        "        # tirage d'un but pour l'épisode\n",
        "        goal_label_agent(train_workspace, t=0, choice=True, n_steps=2)\n",
        "        # train sur le workspace\n",
        "        train_agent(train_workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n",
        "\n",
        "        transition_workspace = train_workspace.get_transitions()\n",
        "\n",
        "        # comptage du nb de step de l'épisode\n",
        "        action = transition_workspace[\"action\"]\n",
        "        nb_steps += action[0].shape[0]\n",
        "\n",
        "        # ajout des transitions au RB\n",
        "        rb.put(transition_workspace)\n",
        "\n",
        "        # 7.1) Loop Replay Buffer\n",
        "        for _ in range(cfg.algorithm.n_updates):\n",
        "            #her_agent(transition_workspace, t=0, rb=rb, n_steps=2)\n",
        "\n",
        "            # tirage aléatoire d'un minibatch dans un Workspace\n",
        "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
        "\n",
        "            # modifie le but atteint\n",
        "            achieved_goal = rb_workspace[\"env/env_obs\"][-1]\n",
        "            goal_label_agent(rb_workspace, t=0, achieved_goal=achieved_goal, n_steps=2)\n",
        "            #goal_label_agent(rb_workspace, t=0, desired_goal=achieved_goal, n_steps=2)\n",
        "            #reward_agent(rb_workspace, t=0, n_steps=2)\n",
        "\n",
        "            # calcul des Q-values\n",
        "            # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace).\n",
        "            q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
        "\n",
        "            q_values, done, truncated, reward, action = rb_workspace[\n",
        "                \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
        "            ]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
        "\n",
        "            target_q_values = rb_workspace[\"q_values\"]\n",
        "            # assert torch.equal(q_values, target_q_values), \"values differ\"\n",
        "\n",
        "            # Determines whether values of the critic should be propagated\n",
        "            # True if the episode reached a time limit or if the task was not done\n",
        "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
        "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "            if rb.size() > cfg.algorithm.learning_starts:\n",
        "                # Compute critic loss\n",
        "                critic_loss = compute_critic_loss(\n",
        "                    cfg, reward, must_bootstrap, q_values[0], target_q_values[1], action\n",
        "                )\n",
        "\n",
        "                # Store the loss for tensorboard display\n",
        "                logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    q_agent.parameters(), cfg.algorithm.max_grad_norm\n",
        "                )\n",
        "                optimizer.step()\n",
        "\n",
        "        # 7.2) Maj du Q_target (sous conditions)\n",
        "        if nb_steps - tmp_steps2 > cfg.algorithm.target_critic_update:\n",
        "            tmp_steps2 = nb_steps\n",
        "            target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
        "        \n",
        "        # 7.3) Évaluation régulère\n",
        "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "            tmp_steps = nb_steps\n",
        "            eval_workspace = Workspace()  # Used for evaluation\n",
        "            eval_agent(\n",
        "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
        "            )\n",
        "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "            mean = rewards.mean()\n",
        "            logger.add_log(\"reward\", mean, nb_steps)\n",
        "            print(f\"reward: {mean}\")\n",
        "            reward_logger.add(nb_steps, mean)\n",
        "            \n",
        "            if cfg.save_best and mean > best_reward:\n",
        "                best_reward = mean\n",
        "                directory = \"./dqn_critic/\"\n",
        "                \n",
        "                if not os.path.exists(directory):\n",
        "                    os.makedirs(directory)\n",
        "                \n",
        "                filename = directory + \"dqn_\" + str(mean.item()) + \".agt\"\n",
        "                eval_agent.save_model(filename)\n",
        "                \n",
        "                if cfg.plot_agents:\n",
        "                    policy = eval_agent.agent.agents[1]\n",
        "                    plot_policy(\n",
        "                        policy,\n",
        "                        eval_env_agent,\n",
        "                        \"./dqn_plots/\",\n",
        "                        cfg.gym_env.env_name,\n",
        "                        best_reward,\n",
        "                        stochastic=False,\n",
        "                    )\n",
        "                    plot_critic(\n",
        "                        policy,\n",
        "                        eval_env_agent,\n",
        "                        \"./dqn_plots/\",\n",
        "                        cfg.gym_env.env_name,\n",
        "                        best_reward,\n",
        "                    )"
      ],
      "metadata": {
        "id": "Yi388sJMkKU1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "u18OyRDdqMBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_loop(cfg):\n",
        "    chrono = Chrono()\n",
        "    logdir = \"./plot/\"\n",
        "\n",
        "    if not os.path.exists(logdir):\n",
        "        os.makedirs(logdir)\n",
        "\n",
        "    reward_logger = RewardLogger(\n",
        "        logdir + \"dqn.steps\", logdir + \"dqn.rwd\"\n",
        "    )\n",
        "\n",
        "    for seed in range(cfg.algorithm.nb_seeds):\n",
        "        #cfg.algorithm.seed = seed\n",
        "        torch.manual_seed(cfg.algorithm.seed)\n",
        "        run_dqn(cfg, reward_logger)\n",
        "\n",
        "        if seed < cfg.algorithm.nb_seeds - 1:\n",
        "            reward_logger.new_episode()\n",
        "\n",
        "    reward_logger.save()\n",
        "    chrono.stop()\n",
        "    plotter = Plotter(logdir + \"dqn.steps\", logdir + \"dqn.rwd\")\n",
        "    plotter.plot_reward(\"dqn\", cfg.gym_env.env_name)"
      ],
      "metadata": {
        "id": "GSN9tztCZX-7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_loop(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "jfewpf6pZgQ2",
        "outputId": "79e9706d-b00e-4705-9d72-27d7598141a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dd0927388787>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-9371f42c964d>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#cfg.algorithm.seed = seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrun_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_seeds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cfb9ad24cbda>\u001b[0m in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, reward_logger)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# tirage d'un but pour l'épisode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgoal_label_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# train sur le workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"env/done\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0m_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_variable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/bbrl/agents/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mworkspace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[Agent.__call__] workspace must not be None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f5245448732e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, desired_goal, achieved_goal, choice, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle_vel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m### TypeError: 'int' object is not callable ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"env/desired_goal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/bbrl/agents/agent.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, index, value)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/bbrl/workspace.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, var_name, t, v, batch_dims)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     def get(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/bbrl/workspace.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, t, value, batch_dims)\u001b[0m\n\u001b[1;32m     34\u001b[0m         ), \"Unable to use batch dimensions with SlicedTemporalTensor\"\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    }
  ]
}